#!/usr/bin/env python

from airflow.configuration import conf 
from airflow import settings
from airflow import utils
from airflow import jobs
from airflow.models import DagBag, TaskInstance, DagPickle

import dateutil.parser
from datetime import datetime
import logging
import os
import signal
import sys

import argparse

# Common help text across subcommands
mark_success_help = "Mark jobs as succeeded without running them"
subdir_help = "File location or directory from which to look for the dag"


def log_to_stdout():
    log = logging.getLogger()
    log.setLevel(logging.DEBUG)
    logformat = logging.Formatter(
        "%(asctime)s - %(name)s - %(levelname)s - %(message)s")
    ch = logging.StreamHandler(sys.stdout)
    ch.setFormatter(logformat)
    log.addHandler(ch)


def backfill(args):
    logging.basicConfig(level=logging.INFO, format=settings.SIMPLE_LOG_FORMAT)
    dagbag = DagBag(args.subdir)
    if args.dag_id not in dagbag.dags:
        raise Exception('dag_id could not be found')
    dag = dagbag.dags[args.dag_id]

    if args.start_date:
        args.start_date = dateutil.parser.parse(args.start_date)
    if args.end_date:
        args.end_date = dateutil.parser.parse(args.end_date)

    if args.task_regex:
        dag = dag.sub_dag(
            task_regex=args.task_regex,
            include_upstream=not args.ignore_dependencies)
    dag.run(
        start_date=args.start_date,
        end_date=args.end_date,
        mark_success=args.mark_success)


def run(args):

    utils.pessimistic_connection_handling()

    # Setting up logging
    directory = conf.get('core', 'BASE_LOG_FOLDER') + \
        "/{args.dag_id}/{args.task_id}".format(args=args)
    if not os.path.exists(directory):
        os.makedirs(directory)
    args.execution_date = dateutil.parser.parse(args.execution_date)
    iso = args.execution_date.isoformat()
    filename = "{directory}/{iso}".format(**locals())
    logging.basicConfig(
        filename=filename, level=logging.INFO,
        format=settings.LOG_FORMAT)
    print("Logging into: " + filename)

    if not args.pickle:
        dagbag = DagBag(args.subdir)
        if args.dag_id not in dagbag.dags:
            raise Exception('dag_id could not be found')
        dag = dagbag.dags[args.dag_id]
        task = dag.get_task(task_id=args.task_id)
    else:
        session = settings.Session()
        dag_pickle = session.query(
            DagPickle).filter(DagPickle.id == args.pickle).all()[0]
        dag = dag_pickle.get_object()
        task = dag.get_task(task_id=args.task_id)

    # TODO: add run_local and fire it with the right executor from run
    ti = TaskInstance(task, args.execution_date)

    # This is enough to fail the task instance
    def signal_handler(signum, frame):
        logging.error("SIGINT (ctrl-c) received".format(args.task_id))
        ti.error(args.execution_date)
        sys.exit()
    signal.signal(signal.SIGINT, signal_handler)

    ti.run(
        mark_success=args.mark_success,
        force=args.force,
        ignore_dependencies=args.ignore_dependencies)


def list_dags(args):
    dagbag = DagBag()
    print("\n".join(sorted(dagbag.dags)))


def list_tasks(args):
    dagbag = DagBag()
    if args.dag_id not in dagbag.dags:
        raise Exception('dag_id could not be found')
    dag = dagbag.dags[args.dag_id]
    if args.tree:
        dag.tree_view()
    else:
        tasks = sorted([t.task_id for t in dag.tasks])
        print("\n".join(sorted(tasks)))


def test(args):
    log_to_stdout()
    args.execution_date = dateutil.parser.parse(args.execution_date)
    dagbag = DagBag(args.subdir)
    if args.dag_id not in dagbag.dags:
        raise Exception('dag_id could not be found')
    dag = dagbag.dags[args.dag_id]
    task = dag.get_task(task_id=args.task_id)
    ti = TaskInstance(task, args.execution_date)
    ti.run(force=True, ignore_dependencies=True, test_mode=True)


def clear(args):
    logging.basicConfig(level=logging.INFO, format=settings.SIMPLE_LOG_FORMAT)
    dagbag = DagBag(args.subdir)

    if args.dag_id not in dagbag.dags:
        raise Exception('dag_id could not be found')
    dag = dagbag.dags[args.dag_id]

    if args.start_date:
        args.start_date = datetime.strptime(args.start_date, '%Y-%m-%d')
    if args.end_date:
        args.end_date = datetime.strptime(args.end_date, '%Y-%m-%d')

    if args.task_regex:
        dag = dag.sub_dag(task_regex=args.task_regex)
    dag.clear(
        start_date=args.start_date,
        end_date=args.end_date,
        upstream=args.upstream,
        downstream=args.downstream,
        only_failed=args.only_failed,
        confirm_prompt=True)


def webserver(args):
    log_to_stdout()
    print(settings.HEADER)
    from airflow.www.app import app
    if args.debug:
        print(
            "Starting the web server on port {0} and host {1}.".format(
                args.port, args.hostname))
        app.run(debug=True, port=args.port, host=args.hostname)
    else:
        print(
            'Running Tornado server on host {host} and port {port}...'.format(
                host=args.hostname, port=args.port))
        from tornado.httpserver import HTTPServer
        from tornado.ioloop import IOLoop
        from tornado import log
        from tornado.wsgi import WSGIContainer

        # simple multi-process server
        server = HTTPServer(WSGIContainer(app))
        server.bind(args.port)
        server.start(0)  # Forks multiple sub-processes
        IOLoop.instance().start()


def master(args):
    job = jobs.MasterJob(args.dag_id, args.subdir)
    job.run()

def serve_logs(args):
    print("Starting flask")
    import flask
    flask_app = flask.Flask(__name__)
    @flask_app.route('/log/<path:filename>')
    def serve_logs(filename):
        conf.get('core', 'BASE_LOG_FOLDER')
        print filename
        return flask.send_from_directory(
            conf.get('core', 'BASE_LOG_FOLDER'), 
            filename,
            mimetype="application/json",
            as_attachment=False)
    print(conf.get('core', 'BASE_LOG_FOLDER'))
    flask_app.run(host='0.0.0.0', port=8990, debug=True)

def serve_logs(args):
    print("Starting flask")
    import flask
    flask_app = flask.Flask(__name__)
    @flask_app.route('/log/<path:filename>')
    def serve_logs(filename):
        conf.get('core', 'BASE_LOG_FOLDER')
        print filename
        return flask.send_from_directory(
            conf.get('core', 'BASE_LOG_FOLDER'), 
            filename,
            mimetype="application/json",
            as_attachment=False)
    print(conf.get('core', 'BASE_LOG_FOLDER'))
    WORKER_LOG_SERVER_PORT = \
        int(conf.get('celery', 'WORKER_LOG_SERVER_PORT'))
    flask_app.run(
        host='0.0.0.0', port=WORKER_LOG_SERVER_PORT, debug=True)


def worker(args):
    import subprocess
    sp = subprocess.Popen("airflow serve_logs", shell=True)
    # Worker to serve static log files through this simple flask app
    from airflow.executors.celery_executor import app as celery_app
    celery_app.start()
    sp.kill()


def initdb(args):

    if raw_input(
            "This will drop exisiting tables if they exist. "
            "Proceed? (y/n)").upper() == "Y":
        logging.basicConfig(level=logging.DEBUG,
                            format=settings.SIMPLE_LOG_FORMAT)

        from airflow import models

        logging.info("Dropping tables that exist")
        models.Base.metadata.drop_all(settings.engine)

        logging.info("Creating all tables")
        models.Base.metadata.create_all(settings.engine)

        # Creating the local_mysql DB connection
        session = settings.Session()
        session.query(models.Connection).delete()
        session.add(
            models.Connection(
                conn_id='local_mysql', conn_type='mysql',
                host='localhost', login='airflow', password='airflow',
                schema='airflow'))
        session.commit()
        session.add(
            models.Connection(
                conn_id='mysql_default', conn_type='mysql',
                host='localhost', login='airflow', password='airflow',
                schema='airflow'))
        session.commit()
        session.add(
            models.Connection(
                conn_id='presto_default', conn_type='presto',
                host='localhost', 
                schema='hive', port=10001))
        session.commit()
        session.add(
            models.Connection(
                conn_id='hive_default', conn_type='hive',
                host='localhost', 
                schema='default', port=10000))
        session.commit()


if __name__ == '__main__':

    logging.root.handlers = []

    parser = argparse.ArgumentParser()
    subparsers = parser.add_subparsers(help='sub-command help')

    ht = "Run subsections of a DAG for a specified date range"
    parser_backfill = subparsers.add_parser('backfill', help=ht)
    parser_backfill.add_argument("dag_id", help="The id of the dag to run")
    parser_backfill.add_argument(
        "-t", "--task_regex",
        help="The regex to filter specific task_ids to backfill (optional)")
    parser_backfill.add_argument(
        "-s", "--start_date", help="Overide start_date YYYY-MM-DD")
    parser_backfill.add_argument(
        "-e", "--end_date", help="Overide end_date YYYY-MM-DD")
    parser_backfill.add_argument(
        "-m", "--mark_success",
        help=mark_success_help, action="store_true")
    parser_backfill.add_argument(
        "-i", "--ignore_dependencies",
        help=(
            "Skip upstream tasks, run only the tasks "
            "matching the regexp. Only works in conjunction with task_regex"),
        action="store_true")
    parser_backfill.add_argument(
        "-sd", "--subdir", help=subdir_help,
        default=conf.get('core', 'DAGS_FOLDER'))
    parser_backfill.set_defaults(func=backfill)

    ht = "Clear a set of task instance, as if they never ran"
    parser_clear = subparsers.add_parser('clear', help=ht)
    parser_clear.add_argument("dag_id", help="The id of the dag to run")
    parser_clear.add_argument(
        "-t", "--task_regex",
        help="The regex to filter specific task_ids to clear (optional)")
    parser_clear.add_argument(
        "-s", "--start_date", help="Overide start_date YYYY-MM-DD")
    parser_clear.add_argument(
        "-e", "--end_date", help="Overide end_date YYYY-MM-DD")
    ht = "Include upstream tasks"
    parser_clear.add_argument(
        "-u", "--upstream", help=ht, action="store_true")
    ht = "Only failed jobs"
    parser_clear.add_argument(
        "-f", "--only_failed", help=ht, action="store_true")
    ht = "Include downstream tasks"
    parser_clear.add_argument(
        "-d", "--downstream", help=ht, action="store_true")
    parser_clear.add_argument(
        "-sd", "--subdir", help=subdir_help,
        default=conf.get('core', 'DAGS_FOLDER'))
    parser_clear.set_defaults(func=clear)

    ht = "Run a single task instance"
    parser_run = subparsers.add_parser('run', help=ht)
    parser_run.add_argument("dag_id", help="The id of the dag to run")
    parser_run.add_argument("task_id", help="The task_id to run")
    parser_run.add_argument(
        "execution_date", help="The execution date to run")
    parser_run.add_argument(
        "-sd", "--subdir", help=subdir_help,
        default=conf.get('core', 'DAGS_FOLDER'))
    parser_run.add_argument(
        "-m", "--mark_success", help=mark_success_help, action="store_true")
    parser_run.add_argument(
        "-f", "--force",
        help="Force a run regardless or previous success",
        action="store_true")
    parser_run.add_argument(
        "-i", "--ignore_dependencies",
        help="Ignore upstream and depends_on_past dependencies",
        action="store_true")
    parser_run.add_argument(
        "-p", "--pickle",
        help="Serialized pickle object of the entire dag (used internally)")
    parser_run.set_defaults(func=run)

    ht = (
        "Test a task instance. This will run a task without checking for "
        "dependencies or recording it's state in the database."
    )
    parser_test = subparsers.add_parser('test', help=ht)
    parser_test.add_argument("dag_id", help="The id of the dag to run")
    parser_test.add_argument("task_id", help="The task_id to run")
    parser_test.add_argument(
        "execution_date", help="The execution date to run")
    parser_test.add_argument(
        "-sd", "--subdir", help=subdir_help,
        default=conf.get('core', 'DAGS_FOLDER'))
    parser_test.set_defaults(func=test)

    ht = "Start a Airflow webserver instance"
    parser_webserver = subparsers.add_parser('webserver', help=ht)
    parser_webserver.add_argument(
        "-p", "--port",
        default=conf.get('server', 'WEB_SERVER_PORT'),
        type=int,
        help="Set the port on which to run the web server")
    parser_webserver.add_argument(
        "-hn", "--hostname",
        default=conf.get('server', 'WEB_SERVER_HOST'),
        help="Set the hostname on which to run the web server")
    ht = "Use the server that ships with Flask in debug mode"
    parser_webserver.add_argument(
        "-d", "--debug", help=ht, action="store_true")
    parser_webserver.set_defaults(func=webserver)

    ht = "Start a master scheduler instance"
    parser_master = subparsers.add_parser('master', help=ht)
    parser_master.add_argument(
        "-d", "--dag_id", help="The id of the dag to run")
    parser_master.add_argument(
        "-sd", "--subdir", help=subdir_help,
        default=conf.get('core', 'DAGS_FOLDER'))
    parser_master.set_defaults(func=master)

    ht = "Initialize and reset the metadata database"
    parser_initdb = subparsers.add_parser('initdb', help=ht)
    parser_initdb.set_defaults(func=initdb)

    ht = "List the DAGs"
    parser_list_dags = subparsers.add_parser('list_dags', help=ht)
    parser_list_dags.set_defaults(func=list_dags)

    ht = "List the tasks whithin a DAG"
    parser_list_tasks = subparsers.add_parser('list_tasks', help=ht)
    parser_list_tasks.add_argument(
        "-t", "--tree", help="Tree view", action="store_true")
    parser_list_tasks.add_argument(
        "dag_id", help="The id of the dag")
    parser_list_tasks.set_defaults(func=list_tasks)

    ht = "Start a Celery worker node"
    parser_worker = subparsers.add_parser('worker', help=ht)
    parser_worker.set_defaults(func=worker)

    ht = "Serve logs generate by worker"
    parser_logs = subparsers.add_parser('serve_logs', help=ht)
    parser_logs.set_defaults(func=serve_logs)

    args = parser.parse_args()
    args.func(args)
