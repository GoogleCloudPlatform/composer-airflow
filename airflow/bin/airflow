#!/usr/bin/env python

from airflow.configuration import conf 
from airflow import settings
from airflow import jobs
from airflow.models import DagBag, TaskInstance, DagPickle

import dateutil.parser
from datetime import datetime
import logging
import os
import signal
import sys

import argparse

# Common help text across subcommands
mark_success_help = "Mark jobs as succeeded without running them"
subdir_help = "File location or directory from which to look for the dag"

def backfill(args):
    logging.basicConfig(level=logging.INFO, format=settings.SIMPLE_LOG_FORMAT)
    dagbag = DagBag(args.subdir)
    if args.dag_id not in dagbag.dags:
        raise Exception('dag_id could not be found')
    dag = dagbag.dags[args.dag_id]

    if args.start_date:
        args.start_date = dateutil.parser.parse(args.start_date)
    if args.end_date:
        args.end_date = dateutil.parser.parse(args.end_date)

    if args.task_regex:
        dag = dag.sub_dag(
            task_regex=args.task_regex,
            include_upstream=not args.ignore_dependencies)
    dag.run(
        start_date=args.start_date,
        end_date=args.end_date,
        mark_success=args.mark_success)


def run(args):

    settings.pessimistic_connection_handling()

    # Setting up logging
    directory = conf.get('core', 'BASE_LOG_FOLDER') + \
        "/{args.dag_id}/{args.task_id}".format(args=args)
    if not os.path.exists(directory):
        os.makedirs(directory)
    args.execution_date = dateutil.parser.parse(args.execution_date)
    iso = args.execution_date.isoformat()
    filename = "{directory}/{iso}".format(**locals())
    logging.basicConfig(
        filename=filename, level=logging.INFO,
        format=settings.LOG_FORMAT)
    print("Logging into: " + filename)

    if not args.pickle:
        dagbag = DagBag(args.subdir)
        if args.dag_id not in dagbag.dags:
            raise Exception('dag_id could not be found')
        dag = dagbag.dags[args.dag_id]
        task = dag.get_task(task_id=args.task_id)
    else:
        session = settings.Session()
        dag_pickle = session.query(
            DagPickle).filter(DagPickle.id == args.pickle).all()[0]
        dag = dag_pickle.get_object()
        task = dag.get_task(task_id=args.task_id)

    # TODO: add run_local and fire it with the right executor from run
    ti = TaskInstance(task, args.execution_date)

    # This is enough to fail the task instance
    def signal_handler(signum, frame):
        logging.error("SIGINT (ctrl-c) received".format(args.task_id))
        ti.error(args.execution_date)
        sys.exit()
    signal.signal(signal.SIGINT, signal_handler)

    ti.run(
        mark_success=args.mark_success,
        force=args.force,
        ignore_dependencies=args.ignore_dependencies)


def clear(args):
    logging.basicConfig(level=logging.INFO, format=settings.SIMPLE_LOG_FORMAT)
    dagbag = DagBag(args.subdir)

    if args.dag_id not in dagbag.dags:
        raise Exception('dag_id could not be found')
    dag = dagbag.dags[args.dag_id]

    if args.start_date:
        args.start_date = datetime.strptime(args.start_date, '%Y-%m-%d')
    if args.end_date:
        args.end_date = datetime.strptime(args.end_date, '%Y-%m-%d')

    if args.task_regex:
        dag = dag.sub_dag(task_regex=args.task_regex)
    dag.clear(
        start_date=args.start_date,
        end_date=args.end_date,
        upstream=args.upstream,
        downstream=args.downstream,
        only_failed=args.only_failed,
        confirm_prompt=True)


def webserver(args):
    from airflow.www.app import app
    print(settings.HEADER)
    if args.debug:
        log = logging.getLogger()
        log.setLevel(logging.DEBUG)
        logformat = logging.Formatter(
            "%(asctime)s - %(name)s - %(levelname)s - %(message)s")
        ch = logging.StreamHandler(sys.stdout)
        ch.setFormatter(logformat)
        log.addHandler(ch)
        logging.info(
            "Starting the web server on port {0} and host {1}.".format(
                args.port, args.hostname))
        app.logger.addHandler(ch)
        app.run(debug=True, port=args.port, host=args.hostname)
    else:
        logging.info(
            'Running Tornado server on host {host} and port {port}...'.format(
                host=args.hostname, port=args.port))
        from tornado.httpserver import HTTPServer
        from tornado.ioloop import IOLoop
        from tornado import log
        from tornado.wsgi import WSGIContainer

        logging.basicConfig(level=logging.DEBUG,
          format=settings.LOG_FORMAT)

        log.enable_pretty_logging()
        # simple multi-process server
        server = HTTPServer(WSGIContainer(app))
        server.bind(args.port)
        server.start(0)  # Forks multiple sub-processes
        IOLoop.instance().start()


def master(args):
    job = jobs.MasterJob(args.dag_id, args.subdir)
    job.run()


def worker(args):
    from airflow.executors.celery_executor import app
    app.start()


def initdb(args):

    if raw_input(
            "This will drop exisiting tables if they exist. "
            "Proceed? (y/n)").upper() == "Y":
        logging.basicConfig(level=logging.DEBUG,
                            format=settings.SIMPLE_LOG_FORMAT)

        from airflow import models

        logging.info("Dropping tables that exist")
        models.Base.metadata.drop_all(settings.engine)

        logging.info("Creating all tables")
        models.Base.metadata.create_all(settings.engine)

        # Creating the local_mysql DB connection
        session = settings.Session()
        session.query(models.DatabaseConnection).delete()
        session.add(
            models.DatabaseConnection(
                db_id='local_mysql', db_type='mysql',
                host='localhost', login='airflow', password='airflow',
                schema='airflow'))
        session.commit()
        session.add(
            models.DatabaseConnection(
                db_id='mysql_default', db_type='mysql',
                host='localhost', login='airflow', password='airflow',
                schema='airflow'))
        session.commit()
        session.add(
            models.DatabaseConnection(
                db_id='presto_default', db_type='presto',
                host='localhost', 
                schema='hive', port=10001))
        session.commit()
        session.add(
            models.DatabaseConnection(
                db_id='hive_default', db_type='hive',
                host='localhost', 
                schema='default', port=10000))
        session.commit()


if __name__ == '__main__':

    logging.root.handlers = []

    parser = argparse.ArgumentParser()
    subparsers = parser.add_subparsers(help='sub-command help')

    ht = "Run subsections of a DAG for a specified date range"
    parser_backfill = subparsers.add_parser('backfill', help=ht)
    parser_backfill.add_argument("dag_id", help="The id of the dag to run")
    parser_backfill.add_argument(
        "-t", "--task_regex",
        help="The regex to filter specific task_ids to backfill (optional)")
    parser_backfill.add_argument(
        "-s", "--start_date", help="Overide start_date YYYY-MM-DD")
    parser_backfill.add_argument(
        "-e", "--end_date", help="Overide end_date YYYY-MM-DD")
    parser_backfill.add_argument(
        "-m", "--mark_success",
        help=mark_success_help, action="store_true")
    parser_backfill.add_argument(
        "-i", "--ignore_dependencies",
        help=(
            "Skip upstream tasks, run only the tasks "
            "matching the regexp. Only works in conjunction with task_regex"),
        action="store_true")
    parser_backfill.add_argument(
        "-sd", "--subdir", help=subdir_help,
        default=conf.get('core', 'DAGS_FOLDER'))
    parser_backfill.set_defaults(func=backfill)

    ht = "Clear a set of task instance, as if they never ran"
    parser_clear = subparsers.add_parser('clear', help=ht)
    parser_clear.add_argument("dag_id", help="The id of the dag to run")
    parser_clear.add_argument(
        "-t", "--task_regex",
        help="The regex to filter specific task_ids to clear (optional)")
    parser_clear.add_argument(
        "-s", "--start_date", help="Overide start_date YYYY-MM-DD")
    parser_clear.add_argument(
        "-e", "--end_date", help="Overide end_date YYYY-MM-DD")
    ht = "Include upstream tasks"
    parser_clear.add_argument(
        "-u", "--upstream", help=ht, action="store_true")
    ht = "Only failed jobs"
    parser_clear.add_argument(
        "-f", "--only_failed", help=ht, action="store_true")
    ht = "Include downstream tasks"
    parser_clear.add_argument(
        "-d", "--downstream", help=ht, action="store_true")
    parser_clear.add_argument(
        "-sd", "--subdir", help=subdir_help,
        default=conf.get('core', 'DAGS_FOLDER'))
    parser_clear.set_defaults(func=clear)

    ht = "Run a single task instance"
    parser_run = subparsers.add_parser('run', help=ht)
    parser_run.add_argument("dag_id", help="The id of the dag to run")
    parser_run.add_argument("task_id", help="The task_id to run")
    parser_run.add_argument(
        "execution_date", help="The execution date to run")
    parser_run.add_argument(
        "-sd", "--subdir", help=subdir_help,
        default=conf.get('core', 'DAGS_FOLDER'))
    parser_run.add_argument(
        "-m", "--mark_success", help=mark_success_help, action="store_true")
    parser_run.add_argument(
        "-f", "--force",
        help="Force a run regardless or previous success",
        action="store_true")
    parser_run.add_argument(
        "-i", "--ignore_dependencies",
        help="Ignore upstream and depends_on_past dependencies",
        action="store_true")
    parser_run.add_argument(
        "-p", "--pickle",
        help="Serialized pickle object of the entire dag (used internally)")
    parser_run.set_defaults(func=run)

    ht = "Start a Airflow webserver instance"
    parser_webserver = subparsers.add_parser('webserver', help=ht)
    parser_webserver.add_argument(
        "-p", "--port",
        default=conf.get('server', 'WEB_SERVER_PORT'),
        type=int,
        help="Set the port on which to run the web server")
    parser_webserver.add_argument(
        "-hn", "--hostname",
        default=conf.get('server', 'WEB_SERVER_HOST'),
        help="Set the hostname on which to run the web server")
    ht = "Use the server that ships with Flask in debug mode"
    parser_webserver.add_argument(
        "-d", "--debug", help=ht, action="store_true")
    parser_webserver.set_defaults(func=webserver)

    ht = "Start a master scheduler instance"
    parser_master = subparsers.add_parser('master', help=ht)
    parser_master.add_argument(
        "-d", "--dag_id", help="The id of the dag to run")
    parser_master.add_argument(
        "-sd", "--subdir", help=subdir_help,
        default=conf.get('core', 'DAGS_FOLDER'))
    parser_master.set_defaults(func=master)

    ht = "Initialize and reset the metadata database"
    parser_initdb = subparsers.add_parser('initdb', help=ht)
    parser_initdb.set_defaults(func=initdb)

    ht = "Start a Celery worker node"
    parser_worker = subparsers.add_parser('worker', help=ht)
    parser_worker.set_defaults(func=worker)

    args = parser.parse_args()
    args.func(args)
