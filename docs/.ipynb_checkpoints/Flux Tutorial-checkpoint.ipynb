{
 "metadata": {
  "name": "",
  "signature": "sha256:acbd69c2f62fd7d821515a4f17aa5c2a5b2761a65341017fd9faed07b5fcbf75"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#Flux tutorial\n",
      "This tutorial walks through some of the fundamental concepts, objects and their usage.\n",
      "\n",
      "##Hooks\n",
      "Hooks are a simple abstraction layer on systems Flux interacts with. You should expect a lot more consistency across hooks than you would with the different systems' underlying libraries. You should also expect a higher level of abstraction.\n",
      "\n",
      "Connection information is stored in the Flux metadata database, so that you don't need to hard code or remember this connection information. In the bellow example, we connect to a MySQL database by specifying the mysql_dbid, which looks up Flux's metadata to get the actual hostname, login, password, and schema name behind the scene.\n",
      "\n",
      "Common methods:\n",
      "* Get a recordset\n",
      "* Extract a csv file\n",
      "* Run a statement\n",
      "* Load into a table from a csv file\n",
      "* Get a pandas dataframe\n",
      "* Get a json blob (array of objects)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from core.hooks import MySqlHook\n",
      "mysql_hook = MySqlHook(mysql_dbid='local_mysql')\n",
      "sql = \"\"\"\n",
      "SELECT table_schema, table_name \n",
      "FROM information_schema.tables \n",
      "WHERE table_schema = 'flux'\n",
      "\"\"\"\n",
      "mysql_hook.get_records(sql)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 5,
       "text": [
        "(('flux', 'dag'),\n",
        " ('flux', 'db_connection'),\n",
        " ('flux', 'deleteme'),\n",
        " ('flux', 'job'),\n",
        " ('flux', 'log'),\n",
        " ('flux', 'task'),\n",
        " ('flux', 'task_instance'),\n",
        " ('flux', 'tmp'),\n",
        " ('flux', 'user'))"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Operators"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Operators allow you to perform a type of interaction with subsystems. There are 3 main families of operator\n",
      "* **Remote execution:** run a Hive statement, run a map-reduce job, run a bash script, ...\n",
      "* **Sensors:** Wait for a Hive partition, wait for MySQL statement to return a row count > 0, wait for a file to land in HDFS\n",
      "* **Data transfers:** Move data from Hive to MySQL, from a file into a Hive table, from Oracle to Hive, ...\n",
      "\n",
      "An operator instance is a task, and it represents a node in the DAG (directed acyclic graph). A task defines a start_date, end_date (None for open ended) and a schedule_interval (say daily or hourly). Actual task runs for a specific schedule time are what we refer to as task instances.\n",
      "\n",
      "Bellow we run a simple remote MySQL statement, over a date range. The task.run() method will instanciate many task runs for the schedule specified, and run them, storing the state in the Flux database. If you were to re-run this, it would say it already succeded."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from core.operators import MySqlOperator\n",
      "from datetime import datetime, timedelta\n",
      "\n",
      "sql = \"\"\"\n",
      "INSERT INTO tmp\n",
      "SELECT 1;\n",
      "\"\"\"\n",
      "mysql_op = MySqlOperator(task_id='test_task3', sql=sql, mysql_dbid='local_mysql', owner='max')\n",
      "mysql_op.run(start_date=datetime(2014, 9, 15))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Creating a DAG"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "A DAG is simply a collection of tasks, with relationship between them, and their associated task instance run states."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from core.operators import MySqlOperator\n",
      "from core import DAG\n",
      "from datetime import datetime\n",
      "\n",
      "# Setting some default operator parameters\n",
      "default_args = {\n",
      "    'owner': 'max',\n",
      "    'mysql_dbid': 'local_mysql',\n",
      "}\n",
      "\n",
      "# Initializing a directed acyclic graph\n",
      "dag = DAG(dag_id='test_dag')\n",
      "\n",
      "# MySQL Operator \n",
      "sql = \"TRUNCATE TABLE tmp;\"\n",
      "mysql_fisrt = MySqlOperator(task_id='mysql_fisrt', sql=sql, **default_args)\n",
      "dag.add_task(mysql_fisrt)\n",
      "\n",
      "sql = \"\"\"\n",
      "INSERT INTO tmp\n",
      "SELECT {{ macros.random() * 100 }};\n",
      "\"\"\"\n",
      "mysql_second = MySqlOperator(task_id='mysql_second', sql=sql, **default_args)\n",
      "dag.add_task(mysql_second)\n",
      "mysql_second.set_upstream(mysql_fisrt)\n",
      " \n",
      "dag.tree_view()\n",
      "dag.run(start_date=datetime(2014, 9, 1), end_date=datetime(2014, 9, 1))\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "<Task(MySqlOperator): mysql_second>\n",
        "    <Task(MySqlOperator): mysql_fisrt>\n"
       ]
      },
      {
       "ename": "AttributeError",
       "evalue": "'DAG' object has no attribute 'dagbag'",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
        "\u001b[1;32m<ipython-input-8-dc6593b5029f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[0mdag\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtree_view\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m \u001b[0mdag\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstart_date\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2014\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m9\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend_date\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2014\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m9\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
        "\u001b[1;32m/home/mistercrunch/Flux/core/models.pyc\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, start_date, end_date, mark_success)\u001b[0m\n\u001b[0;32m    849\u001b[0m         \u001b[0msession\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msettings\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    850\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 851\u001b[1;33m         \u001b[0mjob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mJob\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexecutor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecutor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    852\u001b[0m         \u001b[0msession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    853\u001b[0m         \u001b[0msession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcommit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;31mAttributeError\u001b[0m: 'DAG' object has no attribute 'dagbag'"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Templating with Jinja"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Jinja is a powerful templating engine in Python. It allows to nicely integrate code logic, variables and call methods whithin your commands. \n",
      "\n",
      "By default all templated fields in operators get access to these objects:\n",
      "* **task_instance** object with execution_date \n",
      "* **macros**, a growing collection of useful methods\n",
      "* **params**, a flexible reference which you pass as you construct a task. You typically would pass it a dictionary of constants, but you are free to pass an entier module or any object here. Params is the Trojan horse from which you pass parameters from your DAG code to your template.\n",
      "* **dag**, a reference to the current DAG object\n",
      "* **task**, a reference to the current task object"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Arbitrary macros\n",
      "sql = \"\"\"\n",
      "INSERT INTO tmp\n",
      "SELECT {{ macros.random() * 100 }} \n",
      "FROM t \n",
      "WHERE ds='{{ macros.hive.latest_partition_for_table(some_other_table) }}';\n",
      "\"\"\"\n",
      "\n",
      "# References to constants, execution_date\n",
      "sql = \"\"\"\n",
      "INSERT OVWERWRITE TABLE {{ some_table }} \n",
      "    PARTITON (ds='{{ task_instance.execution_date }}')\n",
      "SELECT field \n",
      "FROM {{ some_other_table }}\n",
      "WHERE ds='{{ macros.latest_partition_for_table(some_other_table) }}';\n",
      "\"\"\"\n",
      "\n",
      "# Code logic\n",
      "sql = \"\"\"\n",
      "INSERT OVWERWRITE TABLE the_table\n",
      "    PARTITON (ds='{{ task_instance.execution_date }}')\n",
      "{% if (mactros.datetime.now() - task_instance.execution_date).days > 90 %}\n",
      "    SELECT * FROM anonymized_table;\n",
      "{% else %}\n",
      "    SELECT * FROM non_anonymized_table;\n",
      "{% endif %}\n",
      "\"\"\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Command Line"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Perform any surgery you need from the command line. Fix false positive, false negative, rerun subsection DAGs. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%bash\n",
      "# Printing the --help for the main and subcommands\n",
      "cd /home/mistercrunch/Flux\n",
      "./flux --help\n",
      "echo ============================================================================\n",
      "./flux run -h\n",
      "echo ============================================================================\n",
      "./flux clear -h\n",
      "echo ============================================================================\n",
      "./flux webserver -h"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "  _____ __                \n",
        "_/ ____\\  |  __ _____  ___\n",
        "\\   __\\|  | |  |  \\  \\/  /\n",
        " |  |  |  |_|  |  />    < \n",
        " |__|  |____/____//__/\\_ \\\n",
        "                        \\/\n",
        "usage: flux.py [-h] {run,clear,webserver} ...\n",
        "\n",
        "positional arguments:\n",
        "  {run,clear,webserver}\n",
        "                        sub-command help\n",
        "    run                 Run subsections of a DAG for a specified date range\n",
        "    clear               Clear a set of task instance, as if they never ran\n",
        "    webserver           Start a Flux webserver instance\n",
        "\n",
        "optional arguments:\n",
        "  -h, --help            show this help message and exit\n",
        "============================================================================\n",
        "  _____ __                \n",
        "_/ ____\\  |  __ _____  ___\n",
        "\\   __\\|  | |  |  \\  \\/  /\n",
        " |  |  |  |_|  |  />    < \n",
        " |__|  |____/____//__/\\_ \\\n",
        "                        \\/\n",
        "usage: flux.py run [-h] [-t TASK_ID] [-s START_DATE] [-e END_DATE] [-m] [-sd SUBDIR] dag_id\n",
        "\n",
        "positional arguments:\n",
        "  dag_id                The id of the dag to run\n",
        "\n",
        "optional arguments:\n",
        "  -h, --help            show this help message and exit\n",
        "  -t TASK_ID, --task_id TASK_ID\n",
        "                        The id of the task to run\n",
        "  -s START_DATE, --start_date START_DATE\n",
        "                        Overide start_date YYYY-MM-DD\n",
        "  -e END_DATE, --end_date END_DATE\n",
        "                        Overide end_date YYYY-MM-DD\n",
        "  -m, --mark_success    Mark jobs as succeeded without running them\n",
        "  -sd SUBDIR, --subdir SUBDIR\n",
        "                        Sub directory from which to look for the dag\n",
        "============================================================================\n",
        "  _____ __                \n",
        "_/ ____\\  |  __ _____  ___\n",
        "\\   __\\|  | |  |  \\  \\/  /\n",
        " |  |  |  |_|  |  />    < \n",
        " |__|  |____/____//__/\\_ \\\n",
        "                        \\/\n",
        "usage: flux.py clear [-h] [-t TASK_ID] [-s START_DATE] [-e END_DATE] [-sd SUBDIR] dag_id\n",
        "\n",
        "positional arguments:\n",
        "  dag_id                The id of the dag to run\n",
        "\n",
        "optional arguments:\n",
        "  -h, --help            show this help message and exit\n",
        "  -t TASK_ID, --task_id TASK_ID\n",
        "                        The id of the task to run (optional)\n",
        "  -s START_DATE, --start_date START_DATE\n",
        "                        Overide start_date YYYY-MM-DD\n",
        "  -e END_DATE, --end_date END_DATE\n",
        "                        Overide end_date YYYY-MM-DD\n",
        "  -sd SUBDIR, --subdir SUBDIR\n",
        "                        File location or directory from which to look for the dag\n",
        "============================================================================\n",
        "  _____ __                \n",
        "_/ ____\\  |  __ _____  ___\n",
        "\\   __\\|  | |  |  \\  \\/  /\n",
        " |  |  |  |_|  |  />    < \n",
        " |__|  |____/____//__/\\_ \\\n",
        "                        \\/\n",
        "usage: flux.py webserver [-h] [-p PORT]\n",
        "\n",
        "optional arguments:\n",
        "  -h, --help            show this help message and exit\n",
        "  -p PORT, --port PORT  Set the port on which to run the web server\n"
       ]
      }
     ],
     "prompt_number": 27
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%bash\n",
      "# Example run command\n",
      "./flux run example_2 -t runme_1 -s 2014-09-01 -e 2014-09-01\n",
      "#./flux clear example_2 -t runme_1 -s 2014-09-01 -e 2014-09-01"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "1: 2014-09-01 00:00:00\n",
        "  _____ __                \n",
        "_/ ____\\  |  __ _____  ___\n",
        "\\   __\\|  | |  |  \\  \\/  /\n",
        " |  |  |  |_|  |  />    < \n",
        " |__|  |____/____//__/\\_ \\\n",
        "                        \\/\n",
        "Executing <Task(BashOperator): runme_1> for 2014-09-01 00:00:00\n",
        "Runnning command: echo \"1: 2014-09-01 00:00:00\"\n"
       ]
      }
     ],
     "prompt_number": 38
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Creating an Operator"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Deriving BaseOperator isn't hard. You should create all the operators your environment needs as building blocks factories for your pipelines.\n",
      "\n",
      "Here's the source for the MySqlOperator"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from core.models import BaseOperator                                            \n",
      "from core.hooks import MySqlHook                                                \n",
      "                                                                                \n",
      "class MySqlOperator(BaseOperator):                                              \n",
      "                                                                                \n",
      "    __mapper_args__ = {'polymorphic_identity': 'MySqlOperator'} # SqlAlchemy artifact                                                                           \n",
      "    template_fields = ('sql',) # the jinja template will be applied to these fields                                                  \n",
      "                                                                                \n",
      "    def __init__(self, sql, mysql_dbid, *args, **kwargs):                       \n",
      "        super(MySqlOperator, self).__init__(*args, **kwargs)                    \n",
      "                                                                                \n",
      "        self.hook = MySqlHook(mysql_dbid=mysql_dbid)                            \n",
      "        self.sql = sql                                                          \n",
      "                                                                                \n",
      "    def execute(self, execution_date):                                          \n",
      "        print('Executing:' + self.sql)                                          \n",
      "        self.hook.run(self.sql)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Executors"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Executors are an abrastraction on top of systems that can run Flux task instances. The default LocalExecutor is a simple implementation of Python's multiprocessing with a simple joinable queue.\n",
      "\n",
      "Arbitrary executors can be derived from BaseExecutor. Expect a Celery, Redis/Mesos and other executors to be created soon. "
     ]
    }
   ],
   "metadata": {}
  }
 ]
}